{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152cf2f7",
   "metadata": {},
   "source": [
    "## Kmeans implementation from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bcd6a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k=10, max_iters=100):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.objective_values = []  # Storing the objective function values\n",
    "\n",
    "    def initialize_centroids(self, data):\n",
    "        \"\"\"\n",
    "        Randomly initialising the starting position of the centroid\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(data.shape[0], self.k, replace=False)\n",
    "        return data[indices]\n",
    "\n",
    "    def cluster_assignment(self, data, centroids):\n",
    "        \"\"\"\n",
    "        Calculating the distance from data points to the closest centroid\n",
    "        \"\"\"\n",
    "        # E-step\n",
    "        distances = cdist(data, centroids, metric='euclidean')\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update_centroids(self, data, labels):\n",
    "        \"\"\"\n",
    "        Recomputing the new position of centroids after revaluating the mean and variances\n",
    "        \"\"\"\n",
    "        # M-step\n",
    "        centroids = np.array([data[labels == i].mean(axis=0) for i in range(self.k)])\n",
    "        return centroids\n",
    "    \n",
    "    def objective(self, data):\n",
    "        \"\"\"\n",
    "        Function to calculate the kmeans objective\n",
    "        \"\"\"\n",
    "        distances = cdist(data, self.centroids, 'euclidean')\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "        return np.sum(min_distances ** 2)\n",
    "    \n",
    "\n",
    "    def fit(self, data):\n",
    "        self.centroids = self.initialize_centroids(data)\n",
    "        for i in range(self.max_iters):\n",
    "            self.labels = self.cluster_assignment(data, self.centroids)\n",
    "            new_centroids = self.update_centroids(data, self.labels)\n",
    "            \n",
    "            # Calculating the objective function after updating centroids\n",
    "            objective_value = self.objective(data)\n",
    "            self.objective_values.append(objective_value)\n",
    "            \n",
    "            # Check for convergence: if centroids don't change, terminate the loop\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "            \n",
    "\n",
    "    \n",
    "# Functions to calculate the purity score and Gini index (clustering evaluation)\n",
    "def purity_score(true_labels, centroid_labels):\n",
    "    # Using contingency matrix for evaluating clustering accuracy\n",
    "    contingency = contingency_matrix(true_labels, centroid_labels)\n",
    "    majority = np.sum(np.amax(contingency, axis=0))\n",
    "    purity = majority / np.sum(contingency)\n",
    "    return purity\n",
    "\n",
    "\n",
    "def gini_index(true_labels, centroid_labels):\n",
    "    contingency = contingency_matrix(true_labels, centroid_labels)\n",
    "    n_samples = np.sum(contingency)\n",
    "    gini = 0\n",
    "\n",
    "    for cluster in range(contingency.shape[1]):\n",
    "        cluster_size = np.sum(contingency[:, cluster])\n",
    "        if cluster_size == 0:\n",
    "            continue\n",
    "\n",
    "        prob = contingency[:, cluster] / cluster_size\n",
    "\n",
    "        # Using a count for gini impurity\n",
    "        count = 1 - np.sum(prob ** 2)\n",
    "\n",
    "        # Updating gini index\n",
    "        gini += (cluster_size / n_samples) * count\n",
    "    return gini\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46114b",
   "metadata": {},
   "source": [
    "# MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f1cd5e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity Score: 0.59248\n",
      "Gini Index: 0.54667\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "# MNIST from keras\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Flatten/normalize the images for clustering\n",
    "flattened_images = train_images.reshape((train_images.shape[0], -1))\n",
    "\n",
    "# Initializing and fitting KMeans model\n",
    "kmeans = KMeans(k=10, max_iters=300)\n",
    "kmeans.fit(flattened_images)\n",
    "\n",
    "# Calculate purity and Gini index\n",
    "purity = purity_score(train_labels, kmeans.labels)\n",
    "print(f\"Purity Score: {purity:.5f}\")\n",
    "\n",
    "gini_index_score = gini_index(train_labels, kmeans.labels)\n",
    "print(f\"Gini Index: {gini_index_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4be2b",
   "metadata": {},
   "source": [
    "## FASHION Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a9fa908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity Score: 0.61380\n",
      "Gini Index: 0.51725\n"
     ]
    }
   ],
   "source": [
    "# FASHION MNIST dataset from keras\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Flatten the images for clustering\n",
    "flattened_images = train_images.reshape((train_images.shape[0], -1))\n",
    "\n",
    "# Initialize and fit KMeans model\n",
    "kmeans = KMeans(k=10, max_iters=300)\n",
    "kmeans.fit(flattened_images)\n",
    "\n",
    "# Calculate purity and Gini index\n",
    "purity = purity_score(train_labels, kmeans.labels)\n",
    "print(f\"Purity Score: {purity:.5f}\")\n",
    "\n",
    "gini_index_score = gini_index(train_labels, kmeans.labels)\n",
    "print(f\"Gini Index: {gini_index_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715c84b",
   "metadata": {},
   "source": [
    "## 20 NG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2e728bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit and transform the dataset\n",
    "X = vectorizer.fit_transform(newsgroups.data)    \n",
    "\n",
    "# Converting sparse matrix to dense matrix\n",
    "X_dense = X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631e874",
   "metadata": {},
   "source": [
    "### Calculating purity and Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e9f5493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity Score: 0.515106303618053\n",
      "Gini Index: 0.5621106918442331\n"
     ]
    }
   ],
   "source": [
    "#Calculating purity and Gini index\n",
    "purity = purity_score(y, kmeans.labels)\n",
    "gini = gini_index(y, kmeans.labels)\n",
    "\n",
    "print(f'Purity Score: {purity}')\n",
    "print(f'Gini Index: {gini}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4b198",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model to return the weights, mean and covariances from generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "669d5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# def load_data(file_path):\n",
    "#     return np.loadtxt(file_path)\n",
    "\n",
    "def initialize_parameters(data, num_components):\n",
    "    \"\"\"\n",
    "    Randomly initialize means, covariances and weights\n",
    "    :param data:\n",
    "    :param num_components:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    means = data[np.random.choice(data.shape[0], num_components, replace=False)]\n",
    "    covariance = [np.cov(data, rowvar=False)] * num_components\n",
    "    \n",
    "    # Initialize weights evenly, assuming each component has an equal prior probability.\n",
    "    # Weight signifies the probability that the data point was produced by a gaussian\n",
    "    weights = np.ones(num_components) / num_components\n",
    "    return means, covariance, weights\n",
    "\n",
    "def e_step(data, weights, means, covariances, num_components):\n",
    "    \"\"\"\n",
    "    Calculating the responsibilities/prior values for the initialised mean and covariances\n",
    "    :param data:\n",
    "    :param weights:\n",
    "    :param means:\n",
    "    :param covariance:\n",
    "    :param num_components:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Array to hold the responsibilities, which is the probability of each each data point belonging to a particular gaussian\n",
    "    prior_pi = np.zeros((data.shape[0], num_components))\n",
    "\n",
    "    for i in range(num_components):\n",
    "        # Calculate responsibilities by multiplying the p.d. of each data point for each Gaussian component by the component's weight.\n",
    "        prior_pi[:, i] = weights[i] * multivariate_normal.pdf(data, mean=means[i], cov=covariances[i])\n",
    "    \n",
    "    # Normalize the responsibilities so that they sum to 1 for each data point across all the components.\n",
    "    prior_pi /= prior_pi.sum(1)[:, np.newaxis]\n",
    "    return prior_pi\n",
    "\n",
    "def m_step(data, prior_pi, num_components):\n",
    "    \"\"\"\n",
    "    Updating the weights, means and covariances based on the new responsibilities/priors\n",
    "    :param data:\n",
    "    :param prior_pi:\n",
    "    :param num_components:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_k = prior_pi.sum(axis=0)\n",
    "    weights = n_k / data.shape[0]\n",
    "    means = np.dot(prior_pi.T, data) / n_k[:, np.newaxis]\n",
    "    covariance = []\n",
    "\n",
    "    for i in range(num_components):\n",
    "        diff_mat = data - means[i]\n",
    "        weighted_diff = diff_mat.T * prior_pi[:, i]\n",
    "        covariance.append(np.dot(weighted_diff, diff_mat) / n_k[i])\n",
    "\n",
    "    return weights, means, covariance\n",
    "\n",
    "def gmm_step(data, num_components, max_iters=100):\n",
    "    \"\"\"\n",
    "    Combining the EM steps\n",
    "    :param data:\n",
    "    :param num_components:\n",
    "    :param max_iters:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    means, covariances, weights = initialize_parameters(data, num_components)\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        prior_pi = e_step(data, weights, means, covariances, num_components)\n",
    "        weights, means, covariances = m_step(data, prior_pi, num_components)\n",
    "\n",
    "    return weights, means, covariances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "361a6f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters for 2 gaussian mixtures:\n",
      "Weights: [0.33479577 0.66520423]\n",
      "Means: [[2.99413182 3.0520966 ]\n",
      " [7.01314831 3.98313418]]\n",
      "Covariances: [array([[1.01023424, 0.02719139],\n",
      "       [0.02719139, 2.93782297]]), array([[0.97475893, 0.49747031],\n",
      "       [0.49747031, 1.0011426 ]])]\n"
     ]
    }
   ],
   "source": [
    "# Loading datasets\n",
    "data_2gauss = np.loadtxt('2gaussian.txt')\n",
    "\n",
    "# GMM for 2 components\n",
    "weights_2gauss, means_2gauss, covs_2gauss = gmm_step(data_2gauss, 2)\n",
    "print(\"Estimated parameters for 2 gaussian mixtures:\")\n",
    "print(\"Weights:\", weights_2gauss)\n",
    "print(\"Means:\", means_2gauss)\n",
    "print(\"Covariances:\", covs_2gauss)\n",
    "\n",
    "# GMM for 3 components\n",
    "# weights_3gauss, means_3gauss, covs_3gauss = gmm_em(data_3gauss, 3)\n",
    "# print(\"\\nEstimated parameters for 3 gaussian mixtures:\")\n",
    "# print(\"Weights:\", weights_3gauss)\n",
    "# print(\"Means:\", means_3gauss)\n",
    "# print(\"Covariances:\", covs_3gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466f163",
   "metadata": {},
   "source": [
    "## GMM implementation building the multivariate gaussian function from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "89a51078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def multivariate_gaussian(x, mean, cov):\n",
    "    \"\"\"\n",
    "    Function to calculate the multivariate gaussian pdf\n",
    "    :param x:\n",
    "    :param mean:\n",
    "    :param cov:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    k = len(mean)\n",
    "    denom = np.sqrt(((2 * np.pi) ** k) * np.linalg.det(cov))\n",
    "    diff = x - mean\n",
    "    numerator = np.exp(-0.5 * np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff))\n",
    "    gaussian = numerator / denom\n",
    "    return gaussian\n",
    "\n",
    "\n",
    "def initialize_parameters(data, num_components):\n",
    "    \"\"\"\n",
    "    Randomly initialize means, covariances and weights\n",
    "    :param data:\n",
    "    :param num_components:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    means = data[np.random.choice(data.shape[0], num_components, replace=False)]\n",
    "    covariance = [np.cov(data, rowvar=False)] * num_components\n",
    "\n",
    "    # Initialize weights evenly, assuming each component has an equal prior probability.\n",
    "    weights = np.ones(num_components) / num_components\n",
    "    return means, covariance, weights\n",
    "\n",
    "\n",
    "def e_step(data, weights, means, covariance, num_components):\n",
    "    \"\"\"\n",
    "    Calculating the responsibilities/prior values for the initialised mean and covariances\n",
    "    :param data:\n",
    "    :param weights:\n",
    "    :param means:\n",
    "    :param covariance:\n",
    "    :param num_components:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    prior_pi = np.zeros((data.shape[0], num_components))\n",
    "\n",
    "    for i in range(num_components):\n",
    "        for point in range(data.shape[0]):\n",
    "            prior_pi[point, i] = weights[i] * multivariate_gaussian(data[point], means[i], covariance[i])\n",
    "\n",
    "    prior_pi /= prior_pi.sum(1)[:, np.newaxis]\n",
    "    return prior_pi\n",
    "\n",
    "\n",
    "def m_step(data, prior_pi, num_components):\n",
    "    \"\"\"\n",
    "    Updating the weights, means and covariances based on the new responsibilities/priors\n",
    "    :param data:\n",
    "    :param prior_pi:\n",
    "    :param num_components:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_k = prior_pi.sum(axis=0)\n",
    "    weights = n_k / data.shape[0]\n",
    "    means = np.dot(prior_pi.T, data) / n_k[:, np.newaxis]\n",
    "    covariance = []\n",
    "\n",
    "    for i in range(num_components):\n",
    "        diff_mat = data - means[i]\n",
    "        weighted_diff = diff_mat.T * prior_pi[:, i]\n",
    "        covariance.append(np.dot(weighted_diff, diff_mat) / n_k[i])\n",
    "\n",
    "    return weights, means, covariance\n",
    "\n",
    "\n",
    "def gmm_step(data, num_components, max_iters=100):\n",
    "    \"\"\"\n",
    "    Combining the EM steps\n",
    "    :param data:\n",
    "    :param num_components:\n",
    "    :param max_iters:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    means, covariances, weights = initialize_parameters(data, num_components)\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        prior_pi = e_step(data, weights, means, covariances, num_components)\n",
    "        weights, means, covariances = m_step(data, prior_pi, num_components)\n",
    "\n",
    "    return weights, means, covariances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe308c3",
   "metadata": {},
   "source": [
    "## GMM for 2 gaussian mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e9aec0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters for 2 gaussian mixtures:\n",
      "Weights: [0.33479577 0.66520423]\n",
      "Means: [[2.99413182 3.0520966 ]\n",
      " [7.01314831 3.98313418]]\n",
      "Covariances: [array([[1.01023424, 0.02719139],\n",
      "       [0.02719139, 2.93782297]]), array([[0.97475893, 0.49747031],\n",
      "       [0.49747031, 1.0011426 ]])]\n"
     ]
    }
   ],
   "source": [
    "# Loading data from text file to numpy array\n",
    "gauss_data = np.loadtxt('2gaussian.txt')\n",
    "\n",
    "\n",
    "# GMM for 2 components\n",
    "gauss_weights_2, gauss_mean_2, gauss_cov_2 = gmm_step(gauss_data, 2)\n",
    "print(\"Estimated parameters for 2 gaussian mixtures:\")\n",
    "print(\"Weights:\", gauss_weights_2)\n",
    "print(\"Means:\", gauss_mean_2)\n",
    "print(\"Covariances:\", gauss_cov_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b1fa4",
   "metadata": {},
   "source": [
    "## GMM for 3 gaussian mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f6604293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated parameters for 3 gaussian mixtures:\n",
      "Weights: [0.49594551 0.20562097 0.29843352]\n",
      "Means: [[5.01177664 7.00151504]\n",
      " [3.03980012 3.04879137]\n",
      " [7.02158525 4.01547353]]\n",
      "Covariances: [array([[0.97965466, 0.18512102],\n",
      "       [0.18512102, 0.97448689]]), array([[1.02858546, 0.02702432],\n",
      "       [0.02702432, 3.38530379]]), array([[0.99037288, 0.50094401],\n",
      "       [0.50094401, 0.99564696]])]\n"
     ]
    }
   ],
   "source": [
    "# Loading data from text file to numpy array\n",
    "gauss_data = np.loadtxt('3gaussian.txt')\n",
    "\n",
    "# GMM for 3 components\n",
    "gauss_weights_3, gauss_mean_3, gauss_cov_3 = gmm_step(gauss_data, 3)\n",
    "print(\"\\nEstimated parameters for 3 gaussian mixtures:\")\n",
    "print(\"Weights:\", gauss_weights_3)\n",
    "print(\"Means:\", gauss_mean_3)\n",
    "print(\"Covariances:\", gauss_cov_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471347f8",
   "metadata": {},
   "source": [
    "## EM algorithm to determine the biased and mixing probability of coin flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "29c02d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated bias probabilities: [0.93172067 0.23670506 0.60986195]\n",
      "Estimated mixing probabilities: [0.17858302 0.30641388 0.5150031 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "\n",
    "def compute_responsibilities(data, num_flips, bias, prior):\n",
    "    \"\"\"\n",
    "    Function to compute the responsibilities of the coin assignments\n",
    "    given the current parameter estimates.\n",
    "    :param data: data array\n",
    "    :param bias: bias probabilities (P)\n",
    "    :param prior: prior probabilities (P_i)\n",
    "    :param num_flips: D\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # E-step\n",
    "    num_heads = np.sum(data, axis=1, keepdims=True)\n",
    "    likelihoods = binom.pmf(num_heads, num_flips, bias)\n",
    "    weighted_likelihoods = likelihoods * prior\n",
    "    normalized_likelihoods = weighted_likelihoods / np.sum(weighted_likelihoods, axis=1, keepdims=True)\n",
    "    return normalized_likelihoods\n",
    "\n",
    "def update_parameters(data, num_flips, responsibilities):\n",
    "    \"\"\"\n",
    "    Function to update the P and P_i based on newly computed responsibilities\n",
    "    \"\"\"\n",
    "    # M-step\n",
    "    num_heads = np.sum(data, axis=1)\n",
    "    updated_biases = np.dot(responsibilities.T, num_heads) / (num_flips * np.sum(responsibilities, axis=0))\n",
    "    updated_priors = np.mean(responsibilities, axis=0)\n",
    "    return updated_biases, updated_priors\n",
    "\n",
    "def log_likelihood(data, num_flips, bias, prior):\n",
    "    \"\"\"\n",
    "    Function to compute the log likelihood\n",
    "    \"\"\"\n",
    "    num_heads = np.sum(data, axis=1, keepdims=True)\n",
    "    likelihoods = binom.pmf(num_heads, num_flips, bias)\n",
    "    weighted_likelihoods = likelihoods * prior\n",
    "    total_likelihood = np.sum(weighted_likelihoods, axis=1)\n",
    "    return np.sum(np.log(total_likelihood))\n",
    "\n",
    "def coin_probabilities(data, num_flips, num_coins, max_iter=100, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Implementing the EM algorithm for coin toss\n",
    "    \"\"\"\n",
    "    biases = np.random.rand(num_coins)\n",
    "    priors = np.array([0.33,0.33,0.33])\n",
    "    previous_log_likelihood = -np.inf\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        responsibilities = compute_responsibilities(data, num_flips, biases, priors)\n",
    "        biases, priors = update_parameters(data, num_flips, responsibilities)\n",
    "        current_log_likelihood = log_likelihood(data, num_flips, biases, priors)\n",
    "\n",
    "        if np.abs(current_log_likelihood - previous_log_likelihood) < tolerance:\n",
    "            break\n",
    "        previous_log_likelihood = current_log_likelihood\n",
    "\n",
    "    return biases, priors\n",
    "\n",
    "# Load the coin flip outcomes from the file\n",
    "data = np.loadtxt('coin_flips_outcome.txt', dtype=int)\n",
    "\n",
    "# Running the EM algorithm for coin flips\n",
    "P, P_i = coin_probabilities(data, num_flips=20, num_coins=3)\n",
    "print(\"Estimated bias probabilities:\", P)\n",
    "print(\"Estimated mixing probabilities:\", P_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc4a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
